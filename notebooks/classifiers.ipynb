{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lstm import LSTMNet, LSTMNetParams, prepare_data as prepare_data_lstm\n",
    "from mlp_pytorch import (\n",
    "    Net as TorchMLP,\n",
    "    NetParams as TorchMLPParams,\n",
    "    prepare_data as torch_mlp_prepare,\n",
    ")\n",
    "\n",
    "from encoding import encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/features.csv\")\n",
    "X = df.iloc[:, 2:-1]  # skip index and name\n",
    "\n",
    "y = df[\"label\"]  # 10 genres\n",
    "y, code = encode(y) # encode labels to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NORMALIZE X ####\n",
    "# Normalize so everything is on the same scale.\n",
    "\n",
    "cols = X.columns\n",
    "std_scaler = sklearn.preprocessing.StandardScaler()\n",
    "np_scaled = std_scaler.fit_transform(X)\n",
    "\n",
    "# new data frame with the new scaled data. \n",
    "X = pd.DataFrame(np_scaled, columns = cols)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assess(model, X_train, X_test, y_train, y_test, title=\"Default\"):\n",
    "    \"\"\"\n",
    "    Fit given model and assess its performance regarding accuracy, F1 score, AUC score and\n",
    "    matthews correlation coefficient\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # binarize labels for multi class roc score\n",
    "    labels = list(set(y))\n",
    "    y_test_bin = label_binarize(y_test, classes=labels)\n",
    "    y_pred_bin = label_binarize(y_pred, classes=labels)\n",
    "    roc = roc_auc_score(y_test_bin, y_pred_bin, average=\"weighted\", multi_class=\"ovo\")\n",
    "    matt_cor = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{title}:\\n  Acc: {round(acc, 2)}\\n  F1: {round(f1, 2)}\\n  AUC score: {round(roc, 2)}\\n  Matth. corr. coeff.: {round(matt_cor, 2)}\"\n",
    "    )\n",
    "    return acc, f1, roc, matt_cor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save experiment results\n",
    "df_res = pd.DataFrame(columns=[\"classifier\", \"accuracy\", \"F1\", \"AUC\", \"MCC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log. Regression:\n",
      "  Acc: 0.59\n",
      "  F1: 0.6\n",
      "  AUC score: 0.77\n",
      "  Matth. corr. coeff.: 0.55\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=42, max_iter=500).fit(X_train, y_train)\n",
    "log_reg_acc, log_reg_f1, log_reg_auc, log_reg_mcc = model_assess(\n",
    "    log_reg, X_train, X_test, y_train, y_test, title=\"Log. Regression\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Logistic regression\", log_reg_acc, log_reg_f1, log_reg_auc, log_reg_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian naive bayes:\n",
      "  Acc: 0.45\n",
      "  F1: 0.41\n",
      "  AUC score: 0.7\n",
      "  Matth. corr. coeff.: 0.4\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_acc, gnb_f1, gnb_auc, gnb_mcc = model_assess(\n",
    "    gnb, X_train, X_test, y_train, y_test, title=\"Gaussian naive bayes\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Gaussian naive bayes\", gnb_acc, gnb_f1, gnb_auc, gnb_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC:\n",
      "  Acc: 0.73\n",
      "  F1: 0.74\n",
      "  AUC score: 0.85\n",
      "  Matth. corr. coeff.: 0.7\n"
     ]
    }
   ],
   "source": [
    "svc_clf = SVC(gamma=\"scale\", C=3, kernel=\"rbf\", random_state=42)\n",
    "svc_acc, svc_f1, svc_auc, svc_mcc = model_assess(\n",
    "    svc_clf, X_train, X_test, y_train, y_test, title=\"SVC\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"SVC\", svc_acc, svc_f1, svc_auc, svc_mcc]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest:\n",
      "  Acc: 0.67\n",
      "  F1: 0.66\n",
      "  AUC score: 0.81\n",
      "  Matth. corr. coeff.: 0.63\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_acc, forest_f1, forest_auc, forest_mcc = model_assess(\n",
    "    forest_clf, X_train, X_test, y_train, y_test, title=\"Random forest\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"Random forest\", forest_acc, forest_f1, forest_auc, forest_mcc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "  Acc: 0.7\n",
      "  F1: 0.7\n",
      "  AUC score: 0.83\n",
      "  Matth. corr. coeff.: 0.67\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    booster=\"gbtree\",\n",
    "    learning_rate=0.04,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_acc, xgb_f1, xgb_auc, xgb_mcc = model_assess(\n",
    "    xgb, X_train, X_test, y_train, y_test, title=\"XGBoost\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"XGBoost\", xgb_acc, xgb_f1, xgb_auc, xgb_mcc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost random forests:\n",
      "  Acc: 0.68\n",
      "  F1: 0.67\n",
      "  AUC score: 0.82\n",
      "  Matth. corr. coeff.: 0.64\n"
     ]
    }
   ],
   "source": [
    "xgbrf = XGBRFClassifier(\n",
    "    n_estimators=1000,\n",
    "    booster=\"gbtree\",\n",
    "    learning_rate=0.04,\n",
    "    objective=\"multi:softmax\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgbrf_acc, xgbrf_f1, xgbrf_auc, xgbrf_mcc = model_assess(\n",
    "    xgbrf, X_train, X_test, y_train, y_test, title=\"XGBoost random forests\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"XGBoost random forests\", xgbrf_acc, xgbrf_f1, xgbrf_auc, xgbrf_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Sklearn:\n",
      "  Acc: 0.73\n",
      "  F1: 0.73\n",
      "  AUC score: 0.85\n",
      "  Matth. corr. coeff.: 0.7\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    activation=\"tanh\",\n",
    "    solver=\"adam\",\n",
    "    alpha=0.0001,\n",
    "    learning_rate=\"adaptive\",\n",
    "    learning_rate_init=0.01,\n",
    ")\n",
    "mlp_acc, mlp_f1, mlp_auc, mlp_mcc = model_assess(\n",
    "    mlp, X_train, X_test, y_train, y_test, title=\"MLP Sklearn\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"MLP Sklearn\", mlp_acc, mlp_f1, mlp_auc, mlp_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP PyTorch:\n",
      "  Acc: 0.66\n",
      "  F1: 0.66\n",
      "  AUC score: 0.81\n",
      "  Matth. corr. coeff.: 0.62\n"
     ]
    }
   ],
   "source": [
    "X_train_t, y_train_t, X_test_t, y_test_t = torch_mlp_prepare(\n",
    "    X_train.to_numpy(), y_train, X_test.to_numpy(), y_test\n",
    ")\n",
    "\n",
    "params = TorchMLPParams(\n",
    "    input_features=29, hidden_size=100, num_classes=10, epochs=1000, learning_rate=0.01\n",
    ")\n",
    "mlp_torch = TorchMLP(params)\n",
    "mlp_torch_acc, mlp_torch_f1, mlp_torch_auc, mlp_torch_mcc = model_assess(\n",
    "    mlp_torch, X_train_t, X_test_t, y_train_t, y_test_t, title=\"MLP PyTorch\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\n",
    "    \"MLP PyTorch\",\n",
    "    mlp_torch_acc,\n",
    "    mlp_torch_f1,\n",
    "    mlp_torch_auc,\n",
    "    mlp_torch_mcc,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "  Acc: 0.67\n",
      "  F1: 0.66\n",
      "  AUC score: 0.82\n",
      "  Matth. corr. coeff.: 0.63\n"
     ]
    }
   ],
   "source": [
    "X_train_t, X_test_t, y_train_t, y_test_t = prepare_data_lstm(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "lstm_params = LSTMNetParams(\n",
    "    num_epochs=2000,\n",
    "    learning_rate=0.01,\n",
    "    dropout=0.3,\n",
    "    input_size=29,\n",
    "    hidden_size=20,\n",
    "    hidden_layer=50,\n",
    "    num_layers=1,\n",
    "    num_classes=10,\n",
    "    seq_length=X_train_t.shape[1],\n",
    "    tensorboard=False\n",
    ")\n",
    "lstm = LSTMNet(lstm_params)\n",
    "lstm_acc, lstm_f1, lstm_auc, lstm_mcc = model_assess(\n",
    "    lstm, X_train_t, X_test_t, y_train_t, y_test_t, title=\"LSTM\"\n",
    ")\n",
    "df_res.loc[len(df_res)] = [\"LSTM\", lstm_acc, lstm_f1, lstm_auc, lstm_mcc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv(\"data/classifier_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP Sklearn</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost random forests</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP PyTorch</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian naive bayes</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               classifier  accuracy    F1   AUC   MCC\n",
       "2                     SVC      0.73  0.74  0.85  0.70\n",
       "6             MLP Sklearn      0.73  0.73  0.85  0.70\n",
       "4                 XGBoost      0.70  0.70  0.83  0.67\n",
       "5  XGBoost random forests      0.68  0.67  0.82  0.64\n",
       "8                    LSTM      0.67  0.66  0.82  0.63\n",
       "3           Random forest      0.67  0.66  0.81  0.63\n",
       "7             MLP PyTorch      0.66  0.66  0.81  0.62\n",
       "0     Logistic regression      0.59  0.60  0.77  0.55\n",
       "1    Gaussian naive bayes      0.45  0.41  0.70  0.40"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.sort_values(by=\"MCC\", ascending=False).round(decimals=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "744438a286f552de89f21840df11d95eed1d912f7f5940de34928fec5bf381d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
